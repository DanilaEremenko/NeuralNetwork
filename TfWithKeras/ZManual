--------------------------optimizers--------------------------

class Adadelta: Adadelta optimizer.

class Adagrad: Adagrad optimizer.

class Adam: Adam optimizer.

class Adamax: Adamax optimizer from Adam paper's Section 7.

class Nadam: Nesterov Adam optimizer.

class Optimizer: Abstract optimizer base class.

class RMSprop: RMSProp optimizer.

class SGD: Stochastic gradient descent optimizer.

--------------------------metrics--------------------------

KLD(...)

MAE(...)

MAPE(...)

MSE(...)

MSLE(...)

binary_accuracy(...)

binary_crossentropy(...)

categorical_accuracy(...)

categorical_crossentropy(...)

cosine(...)

cosine_proximity(...)

deserialize(...)

get(...)

hinge(...)

kld(...)

kullback_leibler_divergence(...)

mae(...)

mape(...)

mean_absolute_error(...)

mean_absolute_percentage_error(...)

mean_squared_error(...)

mean_squared_logarithmic_error(...)

mse(...)

msle(...)

poisson(...)

serialize(...)

sparse_categorical_accuracy(...)

sparse_categorical_crossentropy(...)

sparse_top_k_categorical_accuracy(...)

squared_hinge(...)

top_k_categorical_accuracy(...)


--------------------------losses--------------------------

KLD(...)

MAE(...)

MAPE(...)

MSE(...)

MSLE(...)

binary_crossentropy(...)

categorical_crossentropy(...)

categorical_hinge(...)

cosine(...)

cosine_proximity(...)

deserialize(...)

get(...)

hinge(...)

kld(...)

kullback_leibler_divergence(...)

logcosh(...): Logarithm of the hyperbolic cosine of the prediction error.

mae(...)

mape(...)

mean_absolute_error(...)

mean_absolute_percentage_error(...)

mean_squared_error(...)

mean_squared_logarithmic_error(...)

mse(...)

msle(...)

poisson(...)

serialize(...)

sparse_categorical_crossentropy(...)

squared_hinge(...)